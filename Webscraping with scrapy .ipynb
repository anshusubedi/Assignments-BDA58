{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de34fb-caf7-4f9c-8625-9b8720381107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Objective:\n",
    "\n",
    "#The objective of this assignment is to help trainees gain hands-on experience with Scrapy, a powerful web scraping framework in Python. By the end of this assignment, trainees should be able to create Scrapy projects, build spiders to extract data from websites, and store the scraped data in various formats.\n",
    "#Task 1: Install and Set Up Scrapy\n",
    "#Install Scrapy:\n",
    "\n",
    "#Install Scrapy in your Python environment.\n",
    "\n",
    "#Use the following command to install: pip install scrapy\n",
    "!pip install scrapy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf93bf2-32d3-4b94-bf26-5062bc4b81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create a Scrapy Project:\n",
    "\n",
    "#Create a new Scrapy project named \"web_scraper\" in your working directory.\n",
    "\n",
    "!scrapy startproject web_scraper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a293e63-76e4-4bdf-964d-c4b635433fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Create a Spider to Scrape a Website\n",
    "#Choose a Website: Select a simple, publicly accessible website to scrape.\n",
    "\n",
    "!cd web_scrapper && scrapy genspider quotes_spider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231ebb1-4efa-4d58-a041-264a46e9768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data:\n",
    "\n",
    "#Extract the following data from the website:\n",
    "#Quotes: Extract the text of the quotes.\n",
    "#Authors: Extract the name of the author for each quote.\n",
    "#Tags: Extract tags associated with each quote.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Step 1: Specify the URL\n",
    "url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# Step 2: Send the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 3: Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Step 4: Retrieve the HTML content\n",
    "    html_content = response.text\n",
    "    print(\"HTML content retrieved successfully!\")\n",
    "    print(html_content)\n",
    "else:\n",
    "    print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61a9e8-9fe7-4598-ad44-eafc1b40ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 3: Save the Scraped Data\n",
    "\n",
    "#Save Data to a JSON File: Run the spider and save the scraped data to a JSON file.\n",
    "#Save Data to a CSV File: Run the spider again and save the data to a CSV file.\n",
    "\n",
    "!scrapy crawl quotes -o quotes.json\n",
    "!scrapy crawl quotes -o quotes.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a16ba9-50cb-4fc9-b0fc-1008f54fff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 4: Implement Error Handling and Logging\n",
    "\n",
    "#Add Error Handling: Modify your spider to include basic error handling, such as retrying failed requests or skipping certain elements if they are not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6390e2-6fd7-4b95-b284-1ca0719a5b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
